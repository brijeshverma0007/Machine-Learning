{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T00:48:30.837340Z",
     "iopub.status.busy": "2023-10-29T00:48:30.836483Z",
     "iopub.status.idle": "2023-10-29T00:48:30.845229Z",
     "shell.execute_reply": "2023-10-29T00:48:30.843988Z",
     "shell.execute_reply.started": "2023-10-29T00:48:30.837288Z"
    },
    "id": "Hfprm0nVR-Kp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam,RMSprop,Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T00:48:30.847556Z",
     "iopub.status.busy": "2023-10-29T00:48:30.847233Z",
     "iopub.status.idle": "2023-10-29T00:48:30.871795Z",
     "shell.execute_reply": "2023-10-29T00:48:30.870410Z",
     "shell.execute_reply.started": "2023-10-29T00:48:30.847529Z"
    },
    "id": "rvextPbyUNXT"
   },
   "outputs": [],
   "source": [
    "dataSet = pd.read_csv('31-40.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "execution": {
     "iopub.execute_input": "2023-10-29T00:48:30.876602Z",
     "iopub.status.busy": "2023-10-29T00:48:30.875679Z",
     "iopub.status.idle": "2023-10-29T00:48:30.892690Z",
     "shell.execute_reply": "2023-10-29T00:48:30.891257Z",
     "shell.execute_reply.started": "2023-10-29T00:48:30.876549Z"
    },
    "id": "6FGdRp3YUTCy",
    "outputId": "4113b691-68c3-4ecb-a56b-2b623c611ee0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "univ_rank         int64\n",
       "first_initial    object\n",
       "last_initial     object\n",
       "cit_2017          int32\n",
       "cit_2018          int64\n",
       "cit_2019          int64\n",
       "cit_2020          int64\n",
       "cit_2021          int64\n",
       "cit_2022          int64\n",
       "h_index           int64\n",
       "i_10_index        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet['cit_2017'] = dataSet['cit_2017'].astype(str).str.replace(',', '')\n",
    "dataSet['cit_2017'] = dataSet['cit_2017'].astype(str).astype('int')\n",
    "dataSet.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T00:48:30.895349Z",
     "iopub.status.busy": "2023-10-29T00:48:30.894883Z",
     "iopub.status.idle": "2023-10-29T00:48:30.904983Z",
     "shell.execute_reply": "2023-10-29T00:48:30.903612Z",
     "shell.execute_reply.started": "2023-10-29T00:48:30.895307Z"
    }
   },
   "outputs": [],
   "source": [
    "numdata=dataSet.select_dtypes(include=['number'])\n",
    "numdata2=numdata.drop(['h_index','i_10_index', 'univ_rank'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T00:48:30.908705Z",
     "iopub.status.busy": "2023-10-29T00:48:30.908338Z",
     "iopub.status.idle": "2023-10-29T00:48:30.917578Z",
     "shell.execute_reply": "2023-10-29T00:48:30.916278Z",
     "shell.execute_reply.started": "2023-10-29T00:48:30.908674Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features=numdata2.iloc[:,1:6]\n",
    "target= pd.DataFrame(numdata2['cit_2022'], columns = ['cit_2022'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "execution": {
     "iopub.execute_input": "2023-10-29T00:48:30.920616Z",
     "iopub.status.busy": "2023-10-29T00:48:30.919559Z",
     "iopub.status.idle": "2023-10-29T00:48:30.943617Z",
     "shell.execute_reply": "2023-10-29T00:48:30.942408Z",
     "shell.execute_reply.started": "2023-10-29T00:48:30.920571Z"
    },
    "id": "HEoqNofdUaiT",
    "outputId": "394f72ce-6db1-4d1c-e07a-32a0fd8c7040"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cit_2018</th>\n",
       "      <th>cit_2019</th>\n",
       "      <th>cit_2020</th>\n",
       "      <th>cit_2021</th>\n",
       "      <th>cit_2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>108</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>97</td>\n",
       "      <td>119</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>888</td>\n",
       "      <td>794</td>\n",
       "      <td>843</td>\n",
       "      <td>840</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>75</td>\n",
       "      <td>67</td>\n",
       "      <td>59</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151</td>\n",
       "      <td>108</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>54</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>535</td>\n",
       "      <td>497</td>\n",
       "      <td>547</td>\n",
       "      <td>504</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>82</td>\n",
       "      <td>152</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>299</td>\n",
       "      <td>346</td>\n",
       "      <td>357</td>\n",
       "      <td>423</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>81</td>\n",
       "      <td>127</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cit_2018  cit_2019  cit_2020  cit_2021  cit_2022\n",
       "0         22        51        70       108       137\n",
       "1          7        15        97       119       133\n",
       "2        888       794       843       840       755\n",
       "3         76        75        67        59        58\n",
       "4        151       108        95        85        85\n",
       "..       ...       ...       ...       ...       ...\n",
       "95        54        40        45        43        40\n",
       "96       535       497       547       504       505\n",
       "97        52        57        82       152       222\n",
       "98       299       346       357       423       496\n",
       "99        10        36        81       127       147\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "execution": {
     "iopub.execute_input": "2023-10-29T00:48:30.945392Z",
     "iopub.status.busy": "2023-10-29T00:48:30.945035Z",
     "iopub.status.idle": "2023-10-29T00:48:30.960891Z",
     "shell.execute_reply": "2023-10-29T00:48:30.959612Z",
     "shell.execute_reply.started": "2023-10-29T00:48:30.945354Z"
    },
    "id": "UpaI_PiFUbOT",
    "outputId": "ac1b829f-48e1-448f-b8eb-960cac68ab2a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cit_2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cit_2022\n",
       "0        137\n",
       "1        133\n",
       "2        755\n",
       "3         58\n",
       "4         85\n",
       "..       ...\n",
       "95        40\n",
       "96       505\n",
       "97       222\n",
       "98       496\n",
       "99       147\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T00:48:30.963057Z",
     "iopub.status.busy": "2023-10-29T00:48:30.962533Z",
     "iopub.status.idle": "2023-10-29T00:48:30.978542Z",
     "shell.execute_reply": "2023-10-29T00:48:30.977210Z",
     "shell.execute_reply.started": "2023-10-29T00:48:30.963006Z"
    },
    "id": "92Yragy5VhP3"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T00:48:30.981380Z",
     "iopub.status.busy": "2023-10-29T00:48:30.980352Z",
     "iopub.status.idle": "2023-10-29T00:48:30.993067Z",
     "shell.execute_reply": "2023-10-29T00:48:30.991807Z",
     "shell.execute_reply.started": "2023-10-29T00:48:30.981346Z"
    },
    "id": "5b-YwcAPUuOS"
   },
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T00:48:31.032819Z",
     "iopub.status.busy": "2023-10-29T00:48:31.031919Z",
     "iopub.status.idle": "2023-10-29T00:49:22.797876Z",
     "shell.execute_reply": "2023-10-29T00:49:22.796438Z",
     "shell.execute_reply.started": "2023-10-29T00:48:31.032755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Using SGD Optimizer ***\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 265.3703\n",
      "Batch Size: 4, Learning Rate: 0.1, Epochs: 50, Test Loss: 265.3702697753906\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 43.7583\n",
      "Batch Size: 4, Learning Rate: 0.1, Epochs: 100, Test Loss: 43.75829315185547\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 319.6324\n",
      "Batch Size: 4, Learning Rate: 0.01, Epochs: 50, Test Loss: 319.63238525390625\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 86.4993\n",
      "Batch Size: 4, Learning Rate: 0.01, Epochs: 100, Test Loss: 86.49931335449219\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 327.0881\n",
      "Batch Size: 4, Learning Rate: 0.001, Epochs: 50, Test Loss: 327.088134765625\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 326.6750\n",
      "Batch Size: 4, Learning Rate: 0.001, Epochs: 100, Test Loss: 326.6749572753906\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 328.3776\n",
      "Batch Size: 4, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.37762451171875\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 327.9568\n",
      "Batch Size: 4, Learning Rate: 0.0001, Epochs: 100, Test Loss: 327.95684814453125\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 26.8871\n",
      "Batch Size: 8, Learning Rate: 0.1, Epochs: 50, Test Loss: 26.887121200561523\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 53.8853\n",
      "Batch Size: 8, Learning Rate: 0.1, Epochs: 100, Test Loss: 53.885337829589844\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 270.8315\n",
      "Batch Size: 8, Learning Rate: 0.01, Epochs: 50, Test Loss: 270.8314514160156\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 124.9287\n",
      "Batch Size: 8, Learning Rate: 0.01, Epochs: 100, Test Loss: 124.92866516113281\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 328.1513\n",
      "Batch Size: 8, Learning Rate: 0.001, Epochs: 50, Test Loss: 328.15130615234375\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 327.6522\n",
      "Batch Size: 8, Learning Rate: 0.001, Epochs: 100, Test Loss: 327.65216064453125\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 328.6404\n",
      "Batch Size: 8, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.6403503417969\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 328.5520\n",
      "Batch Size: 8, Learning Rate: 0.0001, Epochs: 100, Test Loss: 328.552001953125\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 308.7698\n",
      "Batch Size: 16, Learning Rate: 0.1, Epochs: 50, Test Loss: 308.769775390625\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 76.5377\n",
      "Batch Size: 16, Learning Rate: 0.1, Epochs: 100, Test Loss: 76.53773498535156\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 305.3553\n",
      "Batch Size: 16, Learning Rate: 0.01, Epochs: 50, Test Loss: 305.3553466796875\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 266.3330\n",
      "Batch Size: 16, Learning Rate: 0.01, Epochs: 100, Test Loss: 266.3330078125\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 328.1384\n",
      "Batch Size: 16, Learning Rate: 0.001, Epochs: 50, Test Loss: 328.13836669921875\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 327.7207\n",
      "Batch Size: 16, Learning Rate: 0.001, Epochs: 100, Test Loss: 327.7206726074219\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 328.5881\n",
      "Batch Size: 16, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.588134765625\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 328.5296\n",
      "Batch Size: 16, Learning Rate: 0.0001, Epochs: 100, Test Loss: 328.5295715332031\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 84.3247\n",
      "Batch Size: 32, Learning Rate: 0.1, Epochs: 50, Test Loss: 84.32466125488281\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 138.8242\n",
      "Batch Size: 32, Learning Rate: 0.1, Epochs: 100, Test Loss: 138.82418823242188\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 323.8589\n",
      "Batch Size: 32, Learning Rate: 0.01, Epochs: 50, Test Loss: 323.85888671875\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 291.8411\n",
      "Batch Size: 32, Learning Rate: 0.01, Epochs: 100, Test Loss: 291.8410949707031\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 328.4774\n",
      "Batch Size: 32, Learning Rate: 0.001, Epochs: 50, Test Loss: 328.4773864746094\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 328.3643\n",
      "Batch Size: 32, Learning Rate: 0.001, Epochs: 100, Test Loss: 328.36431884765625\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 328.5682\n",
      "Batch Size: 32, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.5682067871094\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 328.5011\n",
      "Batch Size: 32, Learning Rate: 0.0001, Epochs: 100, Test Loss: 328.5010681152344\n"
     ]
    }
   ],
   "source": [
    "learningRates = [0.1, 0.01, 0.001, 0.0001]\n",
    "batchSizes = [4, 8, 16, 32]\n",
    "epochs = [50, 100]\n",
    "\n",
    "# Using Stochastic Gradient Descent (SGD) Optimizer\n",
    "print('*** Using Stochastic Gradient Descent (SGD) Optimizer ***')\n",
    "for batchSize in batchSizes:\n",
    "    for learning_rate in learningRates:\n",
    "        for epoch in epochs:\n",
    "            model = Sequential()\n",
    "            model.add(Dense(3, input_dim=5, activation='relu'))  \n",
    "            model.add(Dense(1, activation='linear'))  \n",
    "            optimizer = SGD(learning_rate)\n",
    "            model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "            model.fit(features_train, target_train, epochs=epoch, batch_size=batchSize, verbose=0)\n",
    "            loss = model.evaluate(features_test, target_test)\n",
    "            print(f'Batch Size: {batchSize}, Learning Rate: {learning_rate}, Epochs: {epoch}, Test Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T00:49:22.800725Z",
     "iopub.status.busy": "2023-10-29T00:49:22.800347Z",
     "iopub.status.idle": "2023-10-29T00:50:21.004672Z",
     "shell.execute_reply": "2023-10-29T00:50:21.003244Z",
     "shell.execute_reply.started": "2023-10-29T00:49:22.800693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Using ADAM Optimizer ***\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 260.0437\n",
      "Batch Size: 4, Learning Rate: 0.1, Epochs: 50, Test Loss: 260.043701171875\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 12.2171\n",
      "Batch Size: 4, Learning Rate: 0.1, Epochs: 100, Test Loss: 12.217087745666504\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 227.1831\n",
      "Batch Size: 4, Learning Rate: 0.01, Epochs: 50, Test Loss: 227.1830596923828\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 134.7306\n",
      "Batch Size: 4, Learning Rate: 0.01, Epochs: 100, Test Loss: 134.7305908203125\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 320.0014\n",
      "Batch Size: 4, Learning Rate: 0.001, Epochs: 50, Test Loss: 320.00140380859375\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 296.8166\n",
      "Batch Size: 4, Learning Rate: 0.001, Epochs: 100, Test Loss: 296.8165588378906\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 328.5054\n",
      "Batch Size: 4, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.5054016113281\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 328.2589\n",
      "Batch Size: 4, Learning Rate: 0.0001, Epochs: 100, Test Loss: 328.2589416503906\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 289.2514\n",
      "Batch Size: 8, Learning Rate: 0.1, Epochs: 50, Test Loss: 289.2513732910156\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 259.0484\n",
      "Batch Size: 8, Learning Rate: 0.1, Epochs: 100, Test Loss: 259.04840087890625\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 259.3683\n",
      "Batch Size: 8, Learning Rate: 0.01, Epochs: 50, Test Loss: 259.36834716796875\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 223.5240\n",
      "Batch Size: 8, Learning Rate: 0.01, Epochs: 100, Test Loss: 223.52395629882812\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 326.0721\n",
      "Batch Size: 8, Learning Rate: 0.001, Epochs: 50, Test Loss: 326.07208251953125\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 319.8128\n",
      "Batch Size: 8, Learning Rate: 0.001, Epochs: 100, Test Loss: 319.81280517578125\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 328.6230\n",
      "Batch Size: 8, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.62298583984375\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 328.3547\n",
      "Batch Size: 8, Learning Rate: 0.0001, Epochs: 100, Test Loss: 328.35467529296875\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 307.7157\n",
      "Batch Size: 16, Learning Rate: 0.1, Epochs: 50, Test Loss: 307.7156677246094\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 19.0810\n",
      "Batch Size: 16, Learning Rate: 0.1, Epochs: 100, Test Loss: 19.08099937438965\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 326.1708\n",
      "Batch Size: 16, Learning Rate: 0.01, Epochs: 50, Test Loss: 326.1708068847656\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 258.2244\n",
      "Batch Size: 16, Learning Rate: 0.01, Epochs: 100, Test Loss: 258.224365234375\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 328.4000\n",
      "Batch Size: 16, Learning Rate: 0.001, Epochs: 50, Test Loss: 328.3999938964844\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 327.1629\n",
      "Batch Size: 16, Learning Rate: 0.001, Epochs: 100, Test Loss: 327.16290283203125\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 328.6252\n",
      "Batch Size: 16, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.6251525878906\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 328.6459\n",
      "Batch Size: 16, Learning Rate: 0.0001, Epochs: 100, Test Loss: 328.64593505859375\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 315.1430\n",
      "Batch Size: 32, Learning Rate: 0.1, Epochs: 50, Test Loss: 315.14300537109375\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 77.1853\n",
      "Batch Size: 32, Learning Rate: 0.1, Epochs: 100, Test Loss: 77.18525695800781\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 313.5067\n",
      "Batch Size: 32, Learning Rate: 0.01, Epochs: 50, Test Loss: 313.5066833496094\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 275.2140\n",
      "Batch Size: 32, Learning Rate: 0.01, Epochs: 100, Test Loss: 275.2139587402344\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 328.2933\n",
      "Batch Size: 32, Learning Rate: 0.001, Epochs: 50, Test Loss: 328.29327392578125\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 327.5451\n",
      "Batch Size: 32, Learning Rate: 0.001, Epochs: 100, Test Loss: 327.5450744628906\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 328.5875\n",
      "Batch Size: 32, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.58746337890625\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 328.6479\n",
      "Batch Size: 32, Learning Rate: 0.0001, Epochs: 100, Test Loss: 328.64788818359375\n"
     ]
    }
   ],
   "source": [
    "learningRates = [0.1, 0.01, 0.001, 0.0001]\n",
    "batchSizes = [4, 8, 16, 32]\n",
    "epochs = [50, 100]\n",
    "\n",
    "# Using ADAM Optimizer \n",
    "print('*** Using ADAM Optimizer ***')\n",
    "for batchSize in batchSizes:\n",
    "    for learning_rate in learningRates:\n",
    "        for epoch in epochs:\n",
    "            model = Sequential()\n",
    "            model.add(Dense(3, input_dim=5, activation='relu'))  \n",
    "            model.add(Dense(1, activation='linear'))  \n",
    "            optimizer = Adam(learning_rate)\n",
    "            model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "            model.fit(features_train, target_train, epochs=epoch, batch_size=batchSize, verbose=0)\n",
    "            loss = model.evaluate(features_test, target_test)\n",
    "            print(f'Batch Size: {batchSize}, Learning Rate: {learning_rate}, Epochs: {epoch}, Test Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T00:50:21.006732Z",
     "iopub.status.busy": "2023-10-29T00:50:21.006345Z",
     "iopub.status.idle": "2023-10-29T00:51:26.396583Z",
     "shell.execute_reply": "2023-10-29T00:51:26.395424Z",
     "shell.execute_reply.started": "2023-10-29T00:50:21.006701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Using rmsprop Optimizer ***\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 257.2422\n",
      "Batch Size: 4, Learning Rate: 0.1, Epochs: 50, Test Loss: 257.2421569824219\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 14.0012\n",
      "Batch Size: 4, Learning Rate: 0.1, Epochs: 100, Test Loss: 14.001238822937012\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 227.2336\n",
      "Batch Size: 4, Learning Rate: 0.01, Epochs: 50, Test Loss: 227.233642578125\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 231.5526\n",
      "Batch Size: 4, Learning Rate: 0.01, Epochs: 100, Test Loss: 231.5525665283203\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 323.4943\n",
      "Batch Size: 4, Learning Rate: 0.001, Epochs: 50, Test Loss: 323.49432373046875\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 326.6479\n",
      "Batch Size: 4, Learning Rate: 0.001, Epochs: 100, Test Loss: 326.6478576660156\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 328.4232\n",
      "Batch Size: 4, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.4232482910156\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 328.4077\n",
      "Batch Size: 4, Learning Rate: 0.0001, Epochs: 100, Test Loss: 328.40771484375\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 59.6074\n",
      "Batch Size: 8, Learning Rate: 0.1, Epochs: 50, Test Loss: 59.60737991333008\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 13.9807\n",
      "Batch Size: 8, Learning Rate: 0.1, Epochs: 100, Test Loss: 13.980749130249023\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 256.1053\n",
      "Batch Size: 8, Learning Rate: 0.01, Epochs: 50, Test Loss: 256.10528564453125\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 217.8341\n",
      "Batch Size: 8, Learning Rate: 0.01, Epochs: 100, Test Loss: 217.83413696289062\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 327.2797\n",
      "Batch Size: 8, Learning Rate: 0.001, Epochs: 50, Test Loss: 327.27972412109375\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 327.6416\n",
      "Batch Size: 8, Learning Rate: 0.001, Epochs: 100, Test Loss: 327.6416015625\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 328.5159\n",
      "Batch Size: 8, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.5158996582031\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 328.4471\n",
      "Batch Size: 8, Learning Rate: 0.0001, Epochs: 100, Test Loss: 328.4470520019531\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 154.6080\n",
      "Batch Size: 16, Learning Rate: 0.1, Epochs: 50, Test Loss: 154.6080322265625\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 74.6807\n",
      "Batch Size: 16, Learning Rate: 0.1, Epochs: 100, Test Loss: 74.68074035644531\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 303.9168\n",
      "Batch Size: 16, Learning Rate: 0.01, Epochs: 50, Test Loss: 303.91680908203125\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 285.1707\n",
      "Batch Size: 16, Learning Rate: 0.01, Epochs: 100, Test Loss: 285.17071533203125\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 328.1724\n",
      "Batch Size: 16, Learning Rate: 0.001, Epochs: 50, Test Loss: 328.17242431640625\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 327.9384\n",
      "Batch Size: 16, Learning Rate: 0.001, Epochs: 100, Test Loss: 327.93841552734375\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 328.4727\n",
      "Batch Size: 16, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.47265625\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 328.5042\n",
      "Batch Size: 16, Learning Rate: 0.0001, Epochs: 100, Test Loss: 328.504150390625\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 167.1559\n",
      "Batch Size: 32, Learning Rate: 0.1, Epochs: 50, Test Loss: 167.15585327148438\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 76.1741\n",
      "Batch Size: 32, Learning Rate: 0.1, Epochs: 100, Test Loss: 76.17414855957031\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 316.3820\n",
      "Batch Size: 32, Learning Rate: 0.01, Epochs: 50, Test Loss: 316.38201904296875\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 295.9344\n",
      "Batch Size: 32, Learning Rate: 0.01, Epochs: 100, Test Loss: 295.93438720703125\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 327.9649\n",
      "Batch Size: 32, Learning Rate: 0.001, Epochs: 50, Test Loss: 327.9649353027344\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 328.0865\n",
      "Batch Size: 32, Learning Rate: 0.001, Epochs: 100, Test Loss: 328.08648681640625\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 328.4985\n",
      "Batch Size: 32, Learning Rate: 0.0001, Epochs: 50, Test Loss: 328.49847412109375\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 328.5445\n",
      "Batch Size: 32, Learning Rate: 0.0001, Epochs: 100, Test Loss: 328.54449462890625\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "learningRates = [0.1, 0.01, 0.001, 0.0001]\n",
    "batchSizes = [4, 8, 16, 32]\n",
    "epochs = [50, 100]\n",
    "\n",
    "# Using rmsprop Optimizer\n",
    "print('*** Using rmsprop Optimizer ***')\n",
    "for batchSize in batchSizes:\n",
    "    for learning_rate in learningRates:\n",
    "        for epoch in epochs:\n",
    "            model = Sequential()\n",
    "            model.add(Dense(3, input_dim=5, activation='relu'))  \n",
    "            model.add(Dense(1, activation='linear'))  \n",
    "            optimizer = RMSprop(learning_rate)\n",
    "            model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "            model.fit(features_train, target_train, epochs=epoch, batch_size=batchSize, verbose=0)\n",
    "            loss = model.evaluate(features_test, target_test)\n",
    "            print(f'Batch Size: {batchSize}, Learning Rate: {learning_rate}, Epochs: {epoch}, Test Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-RCbIrgcPHe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
